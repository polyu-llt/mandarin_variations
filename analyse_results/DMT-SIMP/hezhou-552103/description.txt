We use an ensemble model of the following classifiers:
1) Pretrained BERT model for Chinese;
2) LSTM with word-embeddings trained on People's Daily News;
3) SVM/Naive Bayes with word ngram and context-free grammar features;
4) Sequential model with global average pooling layer;
5) Word-based bidirectional LSTM.
We tried three ensemble methods:
1) assign the class which has the highest probability (confidence) from any classifier.
2) assign the class with the highest average probability.
3) use an svm to predict the class from the probabilities given by all classifiers. 
