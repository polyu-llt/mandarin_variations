We use a bidirectional long short term memory (BiLSMT) neual network model to generate this submission.

We first use Word2vec method trained on DMT dataset to obtain word embedding matrix, then we can use a word embedding sequence as input sentence representation. A forward and a backwrad LSTM is used to process the sequence and produce hidden states, which contain information from context in two opposite information. After obtaining hidden state sequence, max-over-time pooling operation is applied to form a fix-size vector as sentence representaion, which will be feed into a hidden dense layer with 256 units and a final dense layer to predict.

This BiLSTM model is trained on DMT training data of simplified version with Adam as the optimizer. Learning rate is set to 0.001, batch size is 32. Word embeddings have 300 dimensions and will be fine-tuned during training process.

The macro-weighted f1 score of this model on validation set is 0.9000.